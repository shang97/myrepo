import numpy as np
import matplotlib.pyplot as plt
#import h5py
import warnings
warnings.filterwarnings('ignore')
    
    
# å¤šå±‚ç¥ç»ç½‘ç»œçš„å‚æ•°åˆå§‹åŒ–
def initialize_parameters(layers_dims):
    """
    åˆå§‹åŒ–æ‰€æœ‰å±‚çš„å‚æ•°ï¼ŒåŒ…æ‹¬è¾“å…¥å±‚å’Œéšè—å±‚ä¹‹é—´ï¼Œéšè—å±‚å’Œè¾“å‡ºå±‚ä¹‹é—´ã€‚
    å‚æ•°ï¼š
        layers_dims - æ¯å±‚ç¥ç»å…ƒæ•°é‡çš„åˆ—è¡¨ï¼Œåˆ†åˆ«ä¸º è¾“å…¥å±‚ã€éšè—å±‚ã€è¾“å‡ºå±‚
    """
    np.random.seed(3)
    parameters = {}
    
    # ç¬¬ä¸€ä¸ªæ˜¯è¾“å…¥å±‚çš„ç¥ç»å…ƒæ•°é‡ï¼Œæ‰€ä»¥è®¡ç®—ç¥ç»ç½‘ç»œçš„å±‚æ•°æ—¶è¦å‡ä¸€
    n_layers = len(layers_dims) - 1 
    
    for l in range(1, n_layers+1):
        # l å¯¹åº” layer_dims çš„ç¬¬ l+1 ä¸ªå…ƒç´ 
        parameters["W" + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) / np.sqrt(layers_dims[l - 1]) 
        parameters["b" + str(l)] = np.zeros((layers_dims[l], 1))
        
        # ç¡®ä¿æ•°æ®çš„æ ¼å¼æ˜¯æ­£ç¡®çš„
        assert(parameters["W" + str(l)].shape == (layers_dims[l], layers_dims[l-1]))
        assert(parameters["b" + str(l)].shape == (layers_dims[l], 1))
        
    return parameters


# ä¸åŒçš„æ¿€æ´»å‡½æ•°
def sigmoid(Z):
    return 1 / (1 + np.exp(-Z))

def relu(Z):
    return np.maximum(0, Z)

def tanh():
    pass


# å•å±‚ç½‘ç»œçš„å‰å‘ä¼ æ’­ï¼ŒåŒºåˆ†ä¸åŒå±‚
def forward(A_last, W, b, activation):
    """
    æ ¹æ®ä¸Šä¸€å±‚ï¼ˆl-1ï¼‰çš„çº¿æ€§è¾“å‡ºA_last å’Œæœ¬å±‚çš„å‚æ•° W,bï¼Œè®¡ç®—å‡ºæœ¬å±‚ï¼ˆlï¼‰çš„çº¿æ€§è¾“å‡º Z å’Œæ¿€æ´» A
    è¿”å›ï¼š
         cache - å…ƒç»„ï¼ŒåŒ…å« å…ƒç»„linear_cacheï¼šï¼ˆä¸Šä¸€å±‚çš„çº¿æ€§è¾“å‡º A_lastï¼Œä»¥åŠæœ¬å±‚çš„å‚æ•° Wï¼ˆä¸éœ€è¦ä¿å­˜bï¼‰ï¼Œ
                     å’Œ å…ƒç´  activation_cacheï¼ˆæœ¬å±‚çš„çº¿æ€§è¾“å‡º Zï¼‰ï¼Œ ç”¨äºåå‘ä¼ æ’­
    """
    
    if activation == "sigmoid":
        Z = np.dot(W, A_last) + b
        A = sigmoid(Z)        
    elif activation == "relu":
        Z = np.dot(W, A_last) + b
        A = relu(Z)
    
    assert(A.shape == (W.shape[0], A_last.shape[1]))
    
    linear_cache = (A_last, W, b)
    activation_cache = Z
    cache = (linear_cache, activation_cache)
    
    return A, cache


# å¤šå±‚ç½‘ç»œçš„å‰å‘ä¼ æ’­
def L_layers_forward(X, parameters):
    """
    æ€»å…± Lï¼ˆn_layersï¼‰å±‚ï¼Œå®ç°è¾“å…¥å±‚ï¼ˆXï¼‰åˆ°è¾“å‡ºå±‚ALï¼ˆYhatï¼‰çš„è®¡ç®—
    
    å‚æ•°ï¼š
        parameters - ä»W1 åˆ° WLï¼Œb1 åˆ° bLï¼ˆæ— W0ã€b0ï¼‰
    
    è¿”å›ï¼š
        AL - æœ€åçš„æ¿€æ´»å€¼
        caches - ç¼“å­˜åˆ—è¡¨ï¼Œå…± L ä¸ªcacheå…ƒç»„ï¼šéšè—å±‚çš„ L-1 ä¸ª cacheï¼š(linear_cache, activation_cache)ï¼Œç´¢å¼•ä» 0 åˆ° L-2
                                            å’Œè¾“å‡ºå±‚çš„ä¸€ä¸ªcacheï¼š(linear_cache, activation_cache)ï¼Œç´¢å¼•ä¸º L-1
    """
    # å‚æ•°åŒ…æ‹¬ Wå’Œbå¯¹ï¼Œæ‰€ä»¥è¦é™¤ä»¥2ï¼Œå¦‚æœä¸æ•´é™¤è€Œé‡‡ç”¨é™¤æ³•çš„è¯ï¼Œç»“æœæ˜¯floatï¼Œè€Œrangeå‡½æ•°è¦æ±‚çš„è¾“å…¥å¿…é¡»æ˜¯int
    n_layers = len(parameters) // 2
    
    # æ‰€æœ‰éšè—å±‚ï¼ˆ1 åˆ° L-1å±‚ï¼‰çš„çº¿æ€§è¾“å‡ºå’Œ reluæ¿€æ´»    
    A_last = A0 = X   
    caches = []
    for l in range(1, n_layers): 
        # æ ¹æ®ä¸Šä¸€å±‚çš„æ¿€æ´» A_last å’Œæœ¬å±‚çš„å‚æ•° Wl å’Œ bl æ¥è®¡ç®—æœ¬å±‚ï¼ˆlï¼‰çš„æ¿€æ´»å€¼ A
        A, cache = forward(A_last, parameters['W' + str(l)], parameters['b' + str(l)], "relu")
        caches.append(cache)
        # æœ¬å±‚ A æ˜¯è®¡ç®—ä¸‹ä¸€å±‚ A çš„ A_last
        A_last = A
    
    # è¾“å‡ºå±‚ï¼ˆç¬¬Lå±‚ï¼‰çš„çº¿æ€§è¾“å‡ºå’Œ sigmoidæ¿€æ´»
    AL, cache = forward(A_last, parameters['W' + str(n_layers)], parameters['b' + str(n_layers)], "sigmoid")
    caches.append(cache)
    
    assert(AL.shape == (1, X.shape[1]))    
    
    return AL, caches


# è®¡ç®—æˆæœ¬
def compute_cost_2class(AL, Y):
    """
    äºŒåˆ†ç±»çš„äº¤å‰ç†µæˆæœ¬å‡½æ•°ã€‚

    å‚æ•°ï¼š
        AL - ä¸æ ‡ç­¾é¢„æµ‹ç›¸å¯¹åº”çš„æ¦‚ç‡å‘é‡ï¼Œç»´åº¦ä¸ºï¼ˆ1ï¼Œç¤ºä¾‹æ•°é‡ï¼‰
        Y - æ ‡ç­¾å‘é‡ï¼ˆä¾‹å¦‚ï¼šå¦‚æœä¸æ˜¯çŒ«ï¼Œåˆ™ä¸º0ï¼Œå¦‚æœæ˜¯çŒ«åˆ™ä¸º1ï¼‰ï¼Œç»´åº¦ä¸ºï¼ˆ1ï¼Œæ•°é‡ï¼‰

    è¿”å›ï¼š
        cost - äº¤å‰ç†µæˆæœ¬
    """
    m = Y.shape[1]
    
    # å“ˆè¾¾ç›ç§¯çš„çŸ©é˜µé¡ºåºä¸é‡è¦ï¼ˆelement-wise productï¼‰ï¼› axisé»˜è®¤æ˜¯Noneï¼Œå³å¯¹æ‰€æœ‰çš„å…ƒç´ è¿›è¡Œæ±‚å’Œ
    # print(Y.shape)
    cost = - 1/ m * np.sum(np.multiply(np.log(AL), Y) + np.multiply(np.log(1 - AL), 1 - Y) )
    # print(cost)
    
    # axisé»˜è®¤å¯¹æ‰€æœ‰å…ƒç´ æ±‚å’Œï¼Œæ‰€ä»¥å½“Yæ˜¯å‘é‡æ—¶ï¼Œå¯ä»¥çœ‹ä½œçŸ©é˜µä¹˜æ³•ï¼Œä½†æ˜¯å½“Yæ˜¯çŸ©é˜µï¼ˆå¯¹åº”å¤šåˆ†ç±»ï¼‰å‘¢ï¼Ÿï¼Ÿï¼Ÿ
    # I = np.ones((m, 1))
    # cost1 = -1/m * np.dot( (np.log(AL) * Y + np.log(1 - AL) * (1 - Y)), I )       
    # print(np.squeeze(cost1))
    
    # ç¡®ä¿æˆæœ¬æ˜¯æ ‡é‡
    cost = np.squeeze(cost)
    assert(cost.shape == ())
    
    return cost


# åå‘ä¼ æ’­--çº¿æ€§éƒ¨åˆ†
def linear_backward(dZ, linear_cache):
    """ä¸ºå•å±‚å®ç°åå‘ä¼ æ’­çš„çº¿æ€§éƒ¨åˆ†ï¼ˆç¬¬Lå±‚ï¼‰ã€‚åŸºäº dZl è®¡ç®—å’Œè¾“å‡º dWlã€dbl"""

    A_last, W, b = linear_cache 
    # æ‰¹æ•°æ®é‡
    m = A_last.shape[1]
    
    # å€¼ï¼ˆå‘é‡ï¼‰éƒ½æ˜¯ä¸€æ ·çš„, ä½†æ˜¯äº’ä¸ºè½¬ç½®
    dW = np.dot(dZ, A_last.T) / m
    # dW1 = np.dot(A_prev, dZ.T) / m
    
    # dbçš„ç»“æœæ˜¯ä¸€è‡´çš„ï¼Œæ³¨æ„è¦keepdimsï¼Œä¸è¦squeeze
    db = np.sum(dZ, axis=1, keepdims=True) / m    
    # I = np.ones((m, 1))
    # db1 = np.dot(dZ, I) / m
    
    # æ ¹æ®Wlè®¡ç®—dAl-1
    dA_last = np.dot(W.T, dZ)
    
    assert (dA_last.shape == A_last.shape)
    assert (dW.shape == W.shape)
    # ä¿å­˜bçš„ç›®çš„ï¼Œå°±æ˜¯åœ¨è¿™é‡Œå¯¹ç…§ä¸€ä¸‹shapeï¼Œå…¶ä»–åœ°æ–¹å®Œå…¨ç”¨ä¸åˆ°ï¼Œä½†æ˜¯ä¹Ÿæ˜¯æœ‰å¿…è¦çš„ï¼ï¼ï¼
    assert (db.shape == b.shape)
    
    return dA_last, dW, db


# æ¿€æ´»å‡½æ•°çš„æ±‚å¯¼
def relu_backward(dA, activation_cache): 
    """dZ çš„è®¡ç®—ä¹Ÿå¯ä»¥å†™æˆ Zå’Œæ¿€æ´»å‡½æ•°çš„å¯¼æ•°å€¼ï¼ˆZä¸­çš„å…ƒç´ å¦‚æœå¤§äº0ï¼Œå¯¼æ•°å€¼ä¸º1ï¼Œå…¶ä»–ä¸º0ï¼‰ çš„é€å…ƒç´ è¿ç®—ï¼Œ"""
    Z = activation_cache
    
    dZ = np.array(dA, copy=True) 
    dZ[Z <= 0] = 0

    assert (dZ.shape == Z.shape)
    return dZ


def sigmoid_backward(dA, activation_cache):
    Z = activation_cache
    
    # è¿™ä¸ªé™¤æ³•æ˜¯é€å…ƒç´ é™¤æ³•
    S = 1 / (1 + np.exp(-Z))
    dZ = dA * S * (1- S)

    assert (dZ.shape == Z.shape)
    return dZ

def tanh_backward():
    pass


# å•å±‚ç½‘ç»œçš„åå‘ä¼ æ’­ï¼ŒåŒºåˆ†ä¸åŒå±‚
def linear_activation_backward(dA, cache, activation="relu"):
    # ï¼ˆA_lastï¼ŒWï¼‰ï¼Œ Z
    linear_cache, activation_cache = cache
     
    # åå‘ä¼ æ’­åˆ°ç¬¬ l å±‚ç¥ç»ç½‘ç»œï¼ˆéšè—å±‚ï¼‰
    if activation == "relu":
        # è¾“å…¥dAlï¼Œè®¡ç®—dZl
        dZ = relu_backward(dA, activation_cache)
        # è®¡ç®— ğ‘‘ğ´_ğ‘™ğ‘ğ‘ ğ‘¡ å’Œ ğ‘‘ğ‘Šğ‘™ã€ğ‘‘ğ‘ğ‘™
        dA_last, dW, db = linear_backward(dZ, linear_cache)
    
    # åå‘ä¼ æ’­åˆ°ç¬¬ L å±‚ç¥ç»ç½‘ç»œï¼ˆè¾“å‡ºå±‚ï¼‰
    elif activation == "sigmoid":
        dZ = sigmoid_backward(dA, activation_cache)
        dA_last, dW, db = linear_backward(dZ, linear_cache)
    
    return dA_last, dW, db


# å¤šå±‚ç½‘ç»œçš„åå‘ä¼ æ’­
def L_layers_backward(AL, Y, caches):
    """ ä»æœ€åä¸€å±‚ï¼ˆç¬¬Lå±‚ï¼‰å¼€å§‹ï¼Œå‘å‰åå‘ä¼ æ’­"""
    
    # åˆ—è¡¨cachesçš„æ¯ä¸ªå…ƒç´ æ˜¯æ¯å±‚çš„cacheå…ƒç»„
    n_layers = L =  len(caches)
    m = AL.shape[1]
    
    # é€å…ƒç´ é™¤æ³•ï¼Œå’Œ / çš„ç»“æœä¸€è‡´
    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))
    # dAL1 = - ( Y / AL - (1 - Y) / (1 - AL) )
    
    # å¯¹è¾“å‡ºå±‚çš„åå‘ä¼ æ’­å•ç‹¬è®¡ç®—ï¼Œä¼ å…¥dALï¼Œè®¡ç®— dAL-1 å’Œ dWLã€dbL
    grads = {}
    # åˆ—è¡¨çš„ index éœ€è¦å‡ 1
    current_cache = caches[n_layers - 1]
    grads["dA" + str(n_layers - 1)], grads["dW" + str(n_layers)], grads["db" + str(n_layers)] = linear_activation_backward(dAL, 
                                                                                                    current_cache, "sigmoid")
    
    # å†ä¼ æ’­åˆ°éšè—å±‚ï¼ˆ1 åˆ° L-1å±‚ï¼‰
    for l in reversed(range(1, n_layers)):
        # åˆ—è¡¨çš„ index éœ€è¦å‡ 1
        current_cache = caches[l - 1]        
        # åå‘ä¼ æ’­åˆ°ç¬¬ l å±‚ï¼Œä¼ å…¥ dAl å’Œ Wl bl Al-1ï¼Œè®¡ç®— dZl dWl dbl å’Œ dAl-1 
        dA_last, dW, db = linear_activation_backward(grads["dA" + str(l)], current_cache, "relu")
        grads["dA" + str(l - 1)] = dA_last
        grads["dW" + str(l)] = dW
        grads["db" + str(l)] = db
    
    return grads


# å¤šå±‚ç½‘ç»œçš„å‚æ•°æ›´æ–°
def update_parameters(parameters, grads, learning_rate):
    """æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°å‚æ•°"""
    
    n_layers = L = len(parameters) // 2 # æ•´é™¤ï¼Œç»“æœæ˜¯intè€Œéfloat
    
    # å‚æ•°æ›´æ–°ï¼šä» 1 å±‚ åˆ° L å±‚ï¼ˆåŒ…å«éšè—å±‚å’Œè¾“å‡ºå±‚ï¼‰
    for l in range(1, n_layers + 1):
        parameters["W" + str(l)] = parameters["W" + str(l)] - learning_rate * grads["dW" + str(l)]
        parameters["b" + str(l)] = parameters["b" + str(l)] - learning_rate * grads["db" + str(l)]
        
    return parameters


# å¤šå±‚ç½‘ç»œçš„æ¨¡å‹é¢„æµ‹
def predict(X, y, parameters):
    """ç”¨äºé¢„æµ‹Lå±‚ç¥ç»ç½‘ç»œçš„ç»“æœï¼Œå½“ç„¶ä¹ŸåŒ…å«ä¸¤å±‚ """
    
    # æ‰¹æ•°æ®é‡
    m = X.shape[1]
    p = np.zeros((1, m))
    
    #æ ¹æ®å‚æ•°å‰å‘ä¼ æ’­ï¼ŒALæ˜¯ä¸ªå‘é‡ï¼Œæ¯ä¸ªå…ƒç´ å¯¹åº”æŸä¸ªæ ·æœ¬çš„é¢„æµ‹æ¦‚ç‡
    AL, caches = L_layers_forward(X, parameters)
    
    for i in range(0, m):
        if AL[0, i] > 0.5:
            p[0, i] = 1
        else:
            p[0, i] = 0
    
    print("å‡†ç¡®åº¦ä¸º: "  + str(float(np.sum((p == y))/m)))
        
    return p


# å¤šå±‚ç½‘ç»œçš„ç»¼åˆæ„å»º
def L_layers_model(X, Y, layers_dims, learning_rate=0.01, num_iterations=3000, print_cost=True, isPlot=True):
    """å®ç°ä¸€ä¸ªLå±‚ç¥ç»ç½‘ç»œ"""
    
    np.random.seed(1)
        
    parameters = initialize_parameters(layers_dims)
    
    costs = []
    # å¤šå±‚ç½‘ç»œçš„æ‰¹é‡è®­ç»ƒ
    for i in range(num_iterations + 1):    
        AL, caches = L_layers_forward(X, parameters)        
        cost = compute_cost_2class(AL, Y)     
        grads = L_layers_backward(AL, Y, caches) 
        parameters = update_parameters(parameters, grads, learning_rate)
        costs.append(cost)
        if i % 500 == 0 and print_cost:
            print("ç¬¬", i ,"æ¬¡è¿­ä»£ï¼Œæˆæœ¬å€¼ä¸ºï¼š", cost)
                
    # è¿­ä»£å®Œæˆï¼Œç»˜åˆ¶costå˜åŒ–å›¾
    if isPlot:
        plt.figure(figsize=(6,6))
        plt.plot(costs)
        plt.ylabel('cost')
        plt.xlabel('iterations (per tens)')
        plt.title("Learning rate =" + str(learning_rate))
        plt.show()
    return parameters